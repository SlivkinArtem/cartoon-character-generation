{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAWnYwR8aqaA"
      },
      "outputs": [],
      "source": [
        "# Bringing in tensorflow\n",
        "import tensorflow as tf\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XW3OT0Eu3xR"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import helper\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchvision.utils import save_image\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soXl35eG95fF",
        "outputId": "af0de7f7-1dff-4a34-9186-bac1bd16bc7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "project_path = \"/content/drive/MyDrive/GAN/cartoon\"\n",
        "os.chdir(project_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EV_J50MYF0Ly"
      },
      "outputs": [],
      "source": [
        "train_data_dir = os.path.join(project_path, \"train\")\n",
        "test_data_dir = os.path.join(project_path, \"test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TS2m1y7uPAD"
      },
      "outputs": [],
      "source": [
        "image_size = 64\n",
        "batch_size = 64\n",
        "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qw8QxNLupC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae1ccd95-70f1-4288-c60e-c0063d378692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "trainloader = datasets.ImageFolder(train_data_dir, transform=T.Compose([\n",
        "    T.Resize(image_size),\n",
        "    T.CenterCrop(image_size),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(*stats)]))\n",
        "\n",
        "trainloader = DataLoader(trainloader, batch_size, shuffle=True, num_workers=3, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PBwB5ArvrB6"
      },
      "outputs": [],
      "source": [
        "testloader = datasets.ImageFolder(test_data_dir, transform=T.Compose([\n",
        "    T.Resize(image_size),\n",
        "    T.CenterCrop(image_size),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(*stats)]))\n",
        "\n",
        "testloader = DataLoader(testloader, batch_size, shuffle=True, num_workers=3, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s6xbFxZY0cm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Задание параметров\n",
        "# image_size = 64\n",
        "# batch_size = 64\n",
        "# stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
        "\n",
        "# # Создание загрузчика данных\n",
        "# trainloader = datasets.ImageFolder(\n",
        "#     train_data_dir,\n",
        "#     transform=T.Compose([\n",
        "#         T.Resize(image_size),\n",
        "#         T.CenterCrop(image_size),\n",
        "#         T.ToTensor(),\n",
        "#         T.Normalize(*stats)\n",
        "#     ])\n",
        "# )\n",
        "\n",
        "# trainloader = DataLoader(trainloader, batch_size=batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
        "\n",
        "# # Создание модели TensorFlow\n",
        "# model = tf.keras.Sequential([\n",
        "#     tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "#     tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "#     tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "#     tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "#     tf.keras.layers.Flatten(),\n",
        "#     tf.keras.layers.Dense(64, activation='relu'),\n",
        "#     tf.keras.layers.Dense(10)\n",
        "# ])\n",
        "\n",
        "# # Извлечение изображений из DataLoader и передача в модель\n",
        "# dataiter = iter(trainloader)\n",
        "# images, labels = next(dataiter)\n",
        "\n",
        "# # Конвертация изображений в numpy массив\n",
        "# images_np = images.numpy()\n",
        "\n",
        "# # Транспонирование изображений, чтобы соответствовать TensorFlow\n",
        "# images_np = np.transpose(images_np, (0, 2, 3, 1))\n",
        "\n",
        "# # Передача изображений в модель\n",
        "# output = model(images_np)\n",
        "# print(output)"
      ],
      "metadata": {
        "id": "ODWJr1BNXfxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.compile(optimizer='adam',\n",
        "#               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# epochs = 10\n",
        "# for epoch in range(epochs):\n",
        "#     for images, labels in trainloader:\n",
        "#         images_np = images.numpy()\n",
        "#         images_np = np.transpose(images_np, (0, 2, 3, 1))\n",
        "#         labels_np = labels.numpy()\n",
        "\n",
        "#         model.train_on_batch(images_np, labels_np)\n",
        "#         # Выводим информацию о текущем батче\n",
        "#         print(f\"Epoch {epoch+1}/{epochs}, Batch processed\")"
      ],
      "metadata": {
        "id": "pN2UMkDoY1LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testloader = datasets.ImageFolder(test_data_dir, transform=T.Compose([\n",
        "#     T.Resize(image_size),\n",
        "#     T.CenterCrop(image_size),\n",
        "#     T.ToTensor(),\n",
        "#     T.Normalize(*stats)\n",
        "# ]))\n",
        "\n",
        "# testloader = DataLoader(testloader, batch_size=batch_size, shuffle=False, num_workers=3, pin_memory=True)\n",
        "\n",
        "# for images, labels in testloader:\n",
        "#     images_np = images.numpy()\n",
        "#     images_np = np.transpose(images_np, (0, 2, 3, 1))\n",
        "#     labels_np = labels.numpy()\n",
        "\n",
        "#     test_loss, test_acc = model.evaluate(images_np, labels_np, verbose=2)\n",
        "#     print(f\"Test accuracy: {test_acc}, Test loss: {test_loss}\")\n",
        "#     break  # Удалите этот break для оценки на всем тестовом наборе"
      ],
      "metadata": {
        "id": "CKpTfI-jY79g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfUe5zyXv3n9"
      },
      "outputs": [],
      "source": [
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zy3PKSYZwI4a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def imshow(img,title):\n",
        "    img = img / 2 + 0.5\n",
        "    npimg = img.numpy()\n",
        "\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.title(title)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "for images, labels in dataiter:\n",
        "    # show images\n",
        "    imshow(torchvision.utils.make_grid(images), \"Training images\")\n",
        "    print(\"Images shape:\", images.shape)\n",
        "    print(\"Labels shape:\", labels.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Извлечение изображений из dataiter\n",
        "dataiter = iter(trainloader)\n",
        "images = []\n",
        "for i, batch in enumerate(dataiter):\n",
        "    imgs, _ = batch  # Извлечение изображений, игнорирование меток\n",
        "    images.append(imgs)\n",
        "    if i >= 1:  # Обработаем два батча для проверки\n",
        "        break\n",
        "\n",
        "# Конкатенируем все изображения в один массив\n",
        "images = torch.cat(images)\n",
        "\n",
        "# Преобразование изображений в numpy массив\n",
        "images_np = images.numpy()\n",
        "\n",
        "# Проверим размеры numpy массива\n",
        "print(\"Images numpy shape:\", images_np.shape)"
      ],
      "metadata": {
        "id": "ZpbGeHWjRA87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_np = images.numpy()\n",
        "images_np = np.transpose(images_np, (0, 2, 3, 1))  # Преобразуем из [N, C, H, W] в [N, H, W, C]\n",
        "print(\"Transposed images numpy shape:\", images_np.shape)\n",
        "\n",
        "images_tf = tf.convert_to_tensor(images_np, dtype=tf.float32)\n",
        "print(\"TensorFlow tensor shape:\", images_tf.shape)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(images_tf).batch(64)"
      ],
      "metadata": {
        "id": "HqqPRZCXRY_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxtJdSLe3iPS"
      },
      "source": [
        "Build Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YijwrW3w33Jt"
      },
      "outputs": [],
      "source": [
        "# Bring in the sequential api for the generator and discriminator\n",
        "from tensorflow.keras.models import Sequential\n",
        "# Bring in the layers for the neural network\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTH1345A4AU2"
      },
      "outputs": [],
      "source": [
        "def build_generator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(8 * 8 * 256, input_dim=256))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Reshape((8, 8, 256)))\n",
        "\n",
        "    # Upsampling block 1\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(128, 5, padding='same'))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Upsampling block 2\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(64, 5, padding='same'))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Additional upsampling block to reach (64, 64)\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(32, 5, padding='same'))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Convolutional block 1\n",
        "    model.add(Conv2D(32, 4, padding='same'))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Convolutional block 2\n",
        "    model.add(Conv2D(16, 4, padding='same'))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv layer to get to three channels (RGB)\n",
        "    model.add(Conv2D(3, 3, padding='same', activation='sigmoid'))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejX72SGp6RW_",
        "outputId": "9ee4a669-93b8-497d-be32-334f1ad51477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "Generated Images shape: (4, 64, 64, 3)\n"
          ]
        }
      ],
      "source": [
        "# img = generator.predict(np.random.randn(4,256,1))\n",
        "random_data = np.random.randn(4, 256)\n",
        "generator = build_generator()\n",
        "generated_images = generator.predict(random_data)\n",
        "print(\"Generated Images shape:\", generated_images.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAKB0DJfgNJl",
        "outputId": "b0d6e641-325b-4a86-ee90-326c1bb9b7cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 16384)             4210688   \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 16384)             0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " up_sampling2d (UpSampling2  (None, 16, 16, 256)       0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 16, 16, 128)       819328    \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " up_sampling2d_1 (UpSamplin  (None, 32, 32, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 64)        204864    \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " up_sampling2d_2 (UpSamplin  (None, 64, 64, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 64, 64, 32)        51232     \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 64, 64, 32)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 64, 64, 32)        16416     \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 64, 64, 32)        0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 64, 64, 16)        8208      \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 64, 64, 16)        0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 64, 64, 3)         435       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5311171 (20.26 MB)\n",
            "Trainable params: 5311171 (20.26 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgnqmR-E6Rvs"
      },
      "outputs": [],
      "source": [
        "# Generate\n",
        "img = generator.predict(np.random.randn(4,256))\n",
        "# Setup the subplot formatting\n",
        "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
        "# Loop four times and get images\n",
        "for idx, img in enumerate(img):\n",
        "    # Plot the image using a specific subplot\n",
        "    ax[idx].imshow(np.squeeze(img))\n",
        "    # Appending the image label as the plot title\n",
        "    ax[idx].title.set_text(idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50GudzWfCH3-"
      },
      "source": [
        "Discriminator епта"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbGZ4k8nCQ4D"
      },
      "outputs": [],
      "source": [
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "\n",
        "    # First Conv Block\n",
        "    model.add(Conv2D(32, (5, 5), input_shape=(64, 64, 3), padding='same'))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # Second Conv Block\n",
        "    model.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # Third Conv Block\n",
        "    model.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # Fourth Conv Block\n",
        "    model.add(Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # Flatten then pass to dense layer\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YnA6mfdCjFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c07d7d9-360c-4662-a790-7f6ba44ca2f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminator Input shape: (256, 64, 64, 3)\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Discriminator Output shape: (256, 1)\n"
          ]
        }
      ],
      "source": [
        "generated_images_resized = np.zeros((4, 64, 64, 3))\n",
        "for i in range(4):\n",
        "    generated_images_resized[i] = tf.image.resize(generated_images[i], (64, 64)).numpy()\n",
        "\n",
        "# Создание дискриминатора\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "# Подготовка данных для дискриминатора\n",
        "discriminator_input = np.repeat(generated_images_resized, 64, axis=0)  # Повторяем изображения 64 раза для создания батча\n",
        "print(\"Discriminator Input shape:\", discriminator_input.shape)\n",
        "\n",
        "# Подача изображения в дискриминатор\n",
        "discriminator_output = discriminator.predict(discriminator_input)\n",
        "print(\"Discriminator Output shape:\", discriminator_output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQqbYcvcClQT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a908fed-0d9c-4110-ef03-5ca25588fa73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 64, 64, 32)        2432      \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 64, 64, 32)        0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64, 64, 32)        0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 32, 32, 64)        51264     \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 16, 16, 128)       204928    \n",
            "                                                                 \n",
            " leaky_re_lu_8 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 8, 8, 256)         819456    \n",
            "                                                                 \n",
            " leaky_re_lu_9 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 16384)             0         \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 16384)             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 16385     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1094465 (4.18 MB)\n",
            "Trainable params: 1094465 (4.18 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# discriminator_output"
      ],
      "metadata": {
        "id": "N4KEup9Azgs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dqL4evCCt5i"
      },
      "outputs": [],
      "source": [
        "# img = np.repeat(img[:, :, np.newaxis], 3, axis=2)  # Копируем канал три раза для RGB\n",
        "# img = np.expand_dims(img, axis=0)  # Добавляем размерность батча (батч, высота, ширина, канал)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct Training Loop.\n",
        "Setup Losses and Optimizers"
      ],
      "metadata": {
        "id": "4K8KSKjCdeA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam is going to be the optimizer for both\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# Binary cross entropy is going to be the loss for both\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "\n",
        "g_opt = Adam(learning_rate=0.0001)\n",
        "d_opt = Adam(learning_rate=0.00001)\n",
        "g_loss = BinaryCrossentropy()\n",
        "d_loss = BinaryCrossentropy()"
      ],
      "metadata": {
        "id": "2UDuMiTKdZcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Subclassed Model"
      ],
      "metadata": {
        "id": "a1eNyl7rdxsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the base model class to subclass our training step\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "MIJHBOFmdtUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PumbaGAN(Model):\n",
        "    def __init__(self, generator, discriminator, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "\n",
        "    def compile(self, g_opt, d_opt, g_loss, d_loss, *args, **kwargs):\n",
        "        super().compile(*args, **kwargs)\n",
        "        self.g_opt = g_opt\n",
        "        self.d_opt = d_opt\n",
        "        self.g_loss = g_loss\n",
        "        self.d_loss = d_loss\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        real_images = batch[0]\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, 256))\n",
        "\n",
        "        # Generate fake images\n",
        "        fake_images = self.generator(random_latent_vectors, training=True)\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as d_tape:\n",
        "            yhat_real = self.discriminator(real_images, training=True)\n",
        "            yhat_fake = self.discriminator(fake_images, training=True)\n",
        "            yhat_realfake = tf.concat([yhat_real, yhat_fake], axis=0)\n",
        "            y_realfake = tf.concat([tf.zeros_like(yhat_real), tf.ones_like(yhat_fake)], axis=0)\n",
        "            noise_real = 0.15 * tf.random.uniform(tf.shape(yhat_real))\n",
        "            noise_fake = -0.15 * tf.random.uniform(tf.shape(yhat_fake))\n",
        "            y_realfake += tf.concat([noise_real, noise_fake], axis=0)\n",
        "            total_d_loss = self.d_loss(y_realfake, yhat_realfake)\n",
        "\n",
        "        dgrad = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables)\n",
        "        self.d_opt.apply_gradients(zip(dgrad, self.discriminator.trainable_variables))\n",
        "\n",
        "        # Train the generator\n",
        "        with tf.GradientTape() as g_tape:\n",
        "            gen_images = self.generator(random_latent_vectors, training=True)\n",
        "            predicted_labels = self.discriminator(gen_images, training=False)\n",
        "            total_g_loss = self.g_loss(tf.zeros_like(predicted_labels), predicted_labels)\n",
        "\n",
        "        ggrad = g_tape.gradient(total_g_loss, self.generator.trainable_variables)\n",
        "        self.g_opt.apply_gradients(zip(ggrad, self.generator.trainable_variables))\n",
        "\n",
        "        return {\"d_loss\": total_d_loss, \"g_loss\": total_g_loss}\n"
      ],
      "metadata": {
        "id": "9lLoaIO9d2VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pumba = PumbaGAN(generator, discriminator)"
      ],
      "metadata": {
        "id": "tNM0sZODoYlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pumba.compile(g_opt, d_opt, g_loss, d_loss)"
      ],
      "metadata": {
        "id": "eFVnRi6Mofw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Callback"
      ],
      "metadata": {
        "id": "MR18EwbIyjWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing.image import array_to_img\n",
        "from tensorflow.keras.callbacks import Callback"
      ],
      "metadata": {
        "id": "pH1HqIq7yktV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelMonitor(Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=256, img_size=(64, 64, 3)):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Generate random latent vectors\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "\n",
        "        # Generate images using the generator\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "\n",
        "        # Scale images from [-1, 1] to [0, 1]\n",
        "        generated_images = (generated_images + 1) / 2.0\n",
        "        generated_images = tf.clip_by_value(generated_images, 0, 1)\n",
        "\n",
        "        # Convert to uint8\n",
        "        generated_images = (generated_images * 255).numpy().astype('uint8')\n",
        "\n",
        "        # Save the generated images\n",
        "        for i in range(self.num_img):\n",
        "            img = array_to_img(generated_images[i])\n",
        "            img.save(os.path.join('images', f'generated_img_{epoch}_{i}.png'))\n"
      ],
      "metadata": {
        "id": "qTxrgUOKyn2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "yWbSzXsQzbJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def torch_to_tf_data(loader):\n",
        "    def gen():\n",
        "        for batch in loader:\n",
        "            images, _ = batch\n",
        "            images = images.permute(0, 2, 3, 1).numpy()  # Convert to HWC format\n",
        "            yield images\n",
        "\n",
        "    tf_data = tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        output_signature=tf.TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32)\n",
        "    )\n",
        "\n",
        "    return tf_data\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "tf_train_dataset = torch_to_tf_data(trainloader).batch(batch_size).prefetch(1)\n",
        "tf_test_dataset = torch_to_tf_data(testloader).batch(batch_size).prefetch(1)"
      ],
      "metadata": {
        "id": "_eHX2BbJDJ-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = pumba.fit(tf_train_dataset, epochs=20, callbacks=[ModelMonitor()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X2HAElZcEoLN",
        "outputId": "e98034cc-058d-4baf-8f1f-28244b28873b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:858: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnknownError",
          "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) UNKNOWN:  OSError: Caught OSError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 245, in __getitem__\n    sample = self.loader(path)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 284, in default_loader\n    return pil_loader(path)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 264, in pil_loader\n    return img.convert(\"RGB\")\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 937, in convert\n    self.load()\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\", line 266, in load\n    raise OSError(msg)\nOSError: image file is truncated (5 bytes not processed)\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-22-63a4f3e6781e>\", line 3, in gen\n    for batch in loader:\n\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n    data = self._next_data()\n\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1346, in _next_data\n    return self._process_data(data)\n\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1372, in _process_data\n    data.reraise()\n\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_utils.py\", line 705, in reraise\n    raise exception\n\nOSError: Caught OSError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 245, in __getitem__\n    sample = self.loader(path)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 284, in default_loader\n    return pil_loader(path)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 264, in pil_loader\n    return img.convert(\"RGB\")\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 937, in convert\n    self.load()\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\", line 266, in load\n    raise OSError(msg)\nOSError: image file is truncated (5 bytes not processed)\n\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) UNKNOWN:  OSError: Caught OSError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 245, in __getitem__\n    sample = self.loader(path)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 284, in default_loader\n    return pil_loader(path)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 264, in pil_loader\n    return img.convert(\"RGB\")\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 937, in convert\n    self.load()\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\", line 266, in load\n    raise OSError(msg)\nOSError: image file is truncated (5 bytes not processed)\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-22-63a4f3e6781e>\", line 3, in gen\n    for batch in loader:\n\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n    data = self._next_data()\n\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1346, in _next_data\n    return self._process_data(data)\n\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1372, in _process_data\n    data.reraise()\n\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_utils.py\", line 705, in reraise\n    raise exception\n\nOSError: Caught OSError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 245, in __getitem__\n    sample = self.loader(path)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 284, in default_loader\n    return pil_loader(path)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 264, in pil_loader\n    return img.convert(\"RGB\")\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 937, in convert\n    self.load()\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\", line 266, in load\n    raise OSError(msg)\nOSError: image file is truncated (5 bytes not processed)\n\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_4381]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-87366a29b779>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mModelMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) UNKNOWN:  OSError: Caught OSError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 245, in __getitem__\n    sample = self.loader(path)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 284, in default_loader\n    return pil_loader(path)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 264, in pil_loader\n    return img.convert(\"RGB\")\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 937, in convert\n    self.load()\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\", line 266, in load\n    raise OSError(msg)\nOSError: image file is truncated (5 bytes not processed)\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-22-63a4f3e6781e>\", line 3, in gen\n    for batch in loader:\n\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n    data = self._next_data()\n\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1346, in _next_data\n    return self._process_data(data)\n\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1372, in _process_data\n    data.reraise()\n\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_utils.py\", line 705, in reraise\n    raise exception\n\nOSError: Caught OSError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 245, in __getitem__\n    sample = self.loader(path)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 284, in default_loader\n    return pil_loader(path)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 264, in pil_loader\n    return img.convert(\"RGB\")\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 937, in convert\n    self.load()\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\", line 266, in load\n    raise OSError(msg)\nOSError: image file is truncated (5 bytes not processed)\n\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) UNKNOWN:  OSError: Caught OSError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 245, in __getitem__\n    sample = self.loader(path)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 284, in default_loader\n    return pil_loader(path)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 264, in pil_loader\n    return img.convert(\"RGB\")\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 937, in convert\n    self.load()\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\", line 266, in load\n    raise OSError(msg)\nOSError: image file is truncated (5 bytes not processed)\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-22-63a4f3e6781e>\", line 3, in gen\n    for batch in loader:\n\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n    data = self._next_data()\n\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1346, in _next_data\n    return self._process_data(data)\n\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1372, in _process_data\n    data.reraise()\n\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_utils.py\", line 705, in reraise\n    raise exception\n\nOSError: Caught OSError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 245, in __getitem__\n    sample = self.loader(path)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 284, in default_loader\n    return pil_loader(path)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 264, in pil_loader\n    return img.convert(\"RGB\")\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 937, in convert\n    self.load()\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\", line 266, in load\n    raise OSError(msg)\nOSError: image file is truncated (5 bytes not processed)\n\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_4381]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def preprocess_image(image, label):\n",
        "    try:\n",
        "        image = tf.image.resize(image, [image_size, image_size])\n",
        "        image = (image - 0.5) / 0.5  # Normalize to [-1, 1]\n",
        "        return image, label\n",
        "    except Exception as e:\n",
        "        # Return a blank image and skip this one\n",
        "        image = tf.zeros([image_size, image_size, 3])\n",
        "        return image, label\n",
        "\n",
        "def load_dataset(directory):\n",
        "    dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        directory,\n",
        "        labels='inferred',\n",
        "        label_mode='int',\n",
        "        image_size=(image_size, image_size),\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "train_dataset = load_dataset(train_data_dir)\n",
        "test_dataset = load_dataset(test_data_dir)\n",
        "\n",
        "# Prefetch for performance\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "hist = pumba.fit(train_dataset, epochs=20, callbacks=[ModelMonitor()])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "oC0yj6VNIv74",
        "outputId": "8d9d260f-d342-499d-bab1-e21f7a8d13c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4155 files belonging to 6 classes.\n",
            "Found 512 files belonging to 6 classes.\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Unknown image file format. One of JPEG, PNG, GIF, BMP required.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]]\n\t [[Shape/_8]]\n  (1) INVALID_ARGUMENT:  Unknown image file format. One of JPEG, PNG, GIF, BMP required.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_5996]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-45b34b37eef5>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mModelMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Unknown image file format. One of JPEG, PNG, GIF, BMP required.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]]\n\t [[Shape/_8]]\n  (1) INVALID_ARGUMENT:  Unknown image file format. One of JPEG, PNG, GIF, BMP required.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_5996]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}